"""
–ü—Ä–æ–º–ø—Ç—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–∞–∂–¥—ã–π –∫–ª—é—á
–ø—Ä–µ–¥—Å—Ç–∞–≤—è–ª–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤ —à–∞–±–ª–æ–Ω–µ, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å

–°—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∏–¥–æ–≤ Prompt Templates:

1.String PromptTemplates
-–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–π –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

2.ChatPromptTemplates
-–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

3.MessagesPlaceholder
-–ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—Ç—Å–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –º–µ—Å—Ç–∞ –≤ ChatPromptTemplates

–í—Å–µ –æ–Ω–∏ —Ä–µ–∞–ª–∏–∑—É—é—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Runnable –ø–æ—ç—Ç–æ–º—É –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∑–Ω–∞–∫–æ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
"""


from langchain_core.prompts import PromptTemplate, FewShotChatMessagePromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.messages import HumanMessage
from src.LLMs.init_model import model
from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings, HuggingFaceEmbeddings
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_chroma import Chroma

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")
print(prompt_template.invoke({"topic": "cats"}))
# –ø–æ–¥—Å—Ç–∞–≤–∏—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é. –ü–æ–ª—É—á–∏–º:text='Tell me a joke about cats'

# –ø—Ä–∏–º–µ—Ä ChatPromptTemplates
prompt_template = ChatPromptTemplate([
    ("system", "You should only give answers in Spanish."),
    ("user", "Tell me a joke about {topic}")
])
print('Caht', prompt_template.invoke({"topic": "cats"}))

# –ø—Ä–∏–º–µ—Ä MessagesPlaceholder
prompt_template = ChatPromptTemplate([
    ("system", "You must get answers on Spanish"),
    MessagesPlaceholder("msgs")
])

print('Holder', prompt_template.invoke({'msgs': [HumanMessage(content='Hi'), HumanMessage(content="Hello")]}))
print()

prompt = MessagesPlaceholder("history")
prompt = prompt.format_messages(
    history=[
        ("system", "You must get answers on Spanish"),
        ("human", "Hello")
    ]
)
print('history prompt', prompt)

# –ø—Ä–∏–º–µ—Ä 3
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You must get answers on Spanish"),
        MessagesPlaceholder("history"),
        ("human", "{question}")
    ]
)

print('full history', prompt.invoke(
    {
        "history": [('human', "what is 5 +2?"), ("ai", "5+2 is 7")],
        "question": "now now multiply that by 4"
    }
))

# FewShotChatMessagePromptTemplate
print()
examples = [
    {"input": "2 ü¶ú 2", "output": "4"},
    {"input": "2 ü¶ú 3", "output": "5"},
]

example_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}"),
        ("ai", "{output}"),
    ]
)

few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

print(few_shot_prompt.invoke({}))

# –ø—Ä–∏–º–µ—Ä 2
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)

chain = final_prompt | model
print(chain.invoke({"input": "What is 2 ü¶ú 9?"}))

# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π few-shot prompting
"""
–ú—ã –º–æ–∂–µ–º –∏–º–µ—Ç—å –Ω–∞–±–æ—Ä —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–∞–±–æ—Ç—ã, 
–Ω–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –Ω–∞–º –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –≤—Å–µ. 
–ü–æ—ç—Ç–æ–º—É –º—ã –º–æ–∂–µ–º –≤—ã–±–∏—Ä–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—Ç
–ø–æ–¥ –Ω–∞—à –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞

Chroma  - –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ—Ç–∫—Ä—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º

#install langchain_chroma 
"""
embeddings = HuggingFaceEndpointEmbeddings(model="mixedbread-ai/mxbai-embed-large-v1", task="feature-extraction")
examples = [
    {"input": "2 ü¶ú 2", "output": "4"},
    {"input": "2 ü¶ú 3", "output": "5"},
    {"input": "2 ü¶ú 4", "output": "6"},
    {"input": "7 ü¶ú 5", "output": "12"},
    {"input": "What did the cow say to the moon?", "output": "nothing at all"},
    {
        "input": "Write me a poem about the moon",
        "output": "One for the moon, and one for me, who are we to talk about the moon?",
    },
    {
        "input": "horse",
        "output": "small horse"
    }
]

to_vectorize = [" ".join(example.values()) for example in examples]
vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)

# k - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∏–∂–∞–π—à–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è
example_selector = SemanticSimilarityExampleSelector(
    vectorstore=vectorstore,
    k=2
)
print(example_selector.select_examples({"input": "horse"}))

# —Å–æ–∑–¥–∞–µ–º prompt
few_shot_prompt = FewShotChatMessagePromptTemplate(
    input_variables=["input"],
    example_selector=example_selector,

    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫ –±—É–¥–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä
    # –≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ 1 —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, 1 –æ—Ç–≤–µ—Ç ai

    example_prompt=ChatPromptTemplate.from_messages(
        [("human", "{input}"), ("ai", "{output}")]
    ),
)

print(few_shot_prompt.invoke(input="What is 3 ü¶ú 3").to_messages())

chain = few_shot_prompt | model
print('answer', chain.invoke(input={"input": "what is the result of the expression 5 ü¶ú 7 = ?"}))

# —ç—Ç–æ—Ç prompt –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫ –¥—Ä—É–≥–∏–º
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)
chain = final_prompt | model
print(chain.invoke({"input": "What's 3 ü¶ú 3?"}))  # –µ–±–∞–∞–∞–∞–∞—Ç—å, —É–¥–∞–ª–æ—Å—å
